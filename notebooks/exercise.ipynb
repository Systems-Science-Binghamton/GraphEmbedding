{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a32bb9da",
   "metadata": {},
   "source": [
    "# Exercise \n",
    "\n",
    "In this exercise, we will learn how to compute the spectra of the adjacency matrix by using the the karate club network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09356b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import networkx as nx\n",
    "import igraph\n",
    "\n",
    "G = nx.karate_club_graph()\n",
    "A = nx.to_scipy_sparse_array(G).asfptype()\n",
    "A.data = A.data * 0 + 1  # binarlize\n",
    "\n",
    "src, trg, _ = sparse.find(sparse.triu(A, 1))\n",
    "g = igraph.Graph(tuple(zip(src, trg)), directed=False)\n",
    "\n",
    "labels = np.unique([d[1][\"club\"] for d in G.nodes(data=True)], return_inverse=True)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51889cfa",
   "metadata": {},
   "source": [
    "Let us visualize the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47b882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "ulabels, indices = np.unique(labels, return_inverse=True)\n",
    "node_colors = sns.color_palette(\"Set1\", len(ulabels))\n",
    "node_colors = [node_colors[i] for i in indices]\n",
    "\n",
    "igraph.plot(\n",
    "    g,\n",
    "    vertex_color=node_colors,\n",
    "    vertex_label=np.arange(A.shape[0], dtype=int),\n",
    "    bbox=(300, 300),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbf0962",
   "metadata": {},
   "source": [
    "Let's compute the spectra of the adjacency matrix by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb18d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lam, U = sparse.linalg.eigs(A, k=5, which=\"LM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0670f8bb",
   "metadata": {},
   "source": [
    "# Let's print out the matrix U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519144d0",
   "metadata": {},
   "source": [
    "`scipy.linalg.eigs` produces complex-valued eigenvectors and eigenvalues. When matrix $A$ is symmetric, the eigenvalues and eigenvectors are always real-valued, which is indeed the case for our analysis. Thus, we should take only the real part of these numbers. You can do this by using `np.real`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151b5bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = ...  # convert to real-valued numpy\n",
    "lam = ...  # convert to real-valued numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f7876c",
   "metadata": {},
   "source": [
    "Let's see the eigenvectors in a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7354930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "sns.set(font_scale=1)\n",
    "sns.set_style(\"ticks\")\n",
    "fig, ax = plt.subplots(figsize=(5, 7))\n",
    "\n",
    "sns.heatmap(U, cmap=\"RdBu_r\", center=0, xticklabels=False, yticklabels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a763f2",
   "metadata": {},
   "source": [
    "The left most column corresponds to the principal eigenvector (or eigencentrality). Other embedding captures the community structure of the network. \n",
    "To see this, let's visualize the eigencentrality in the node-link diagram with igraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af0d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "igraph.plot(\n",
    "    g,\n",
    "    vertex_color=node_colors,\n",
    "    vertex_size=...,  # set the vertex size to be proportional to the eigenvector\n",
    "    vertex_label=np.arange(A.shape[0], dtype=int),\n",
    "    bbox=(300, 300),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401a3ce4",
   "metadata": {},
   "source": [
    "Now, let's visualize the second and third eigenvectors by using a scatter plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b1ea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"ticks\")\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=...,  # set the x-axis to be the second eigenvector\n",
    "    y=...,  # set the y-axis to be the thrid eigenvector\n",
    "    color=node_colors,\n",
    ")\n",
    "ax.set_xlabel(\"2nd eigenvector\")\n",
    "ax.set_ylabel(\"3rd eigenvector\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e42719",
   "metadata": {},
   "source": [
    "What does the eigenvalue tell us? It is an importance of each embedding dimension in terms of the matrix reconstruction. Let's see the raw values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83205170",
   "metadata": {},
   "outputs": [],
   "source": [
    "lam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d307fce0",
   "metadata": {},
   "source": [
    "The values are in descending order of the magnitude. The left-most eigenvector, or principal eigenvector, is the most important vector to reconstruct the adjacency matrix, followed by the second and third eigenvectors that capture the community structure in networks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76411ca",
   "metadata": {},
   "source": [
    "The original adjacency matrix can be reconstructed by using the embedding. To see this, let's generate $N$ dimensional embedding, where $N$ is the number of nodes. Since it contains all spectrum, the adjacency matrix should be exactly reconstructed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a167554",
   "metadata": {},
   "outputs": [],
   "source": [
    "lam, U = np.linalg.eig(\n",
    "    A.toarray()\n",
    ")  # We will use numpy.linalg.eig for a full eigendecomposition\n",
    "\n",
    "order = np.argsort(-np.abs(lam))\n",
    "lam, U = lam[order], U[:, order]  # Sort the eigenvalues and eigenvectors by magnitude\n",
    "\n",
    "U = np.real(U)\n",
    "\n",
    "V = A @ U  # Construct the other embedding\n",
    "\n",
    "# Reconstruct the adjacency matrix\n",
    "Ahat = U @ V.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd88fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"ticks\")\n",
    "fig, axes = plt.subplots(figsize=(10, 4), ncols=2)\n",
    "\n",
    "sns.heatmap(A.toarray(), ax=axes[0], cmap=\"cividis\")\n",
    "sns.heatmap(Ahat, ax=axes[1], cmap=\"cividis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aaf5ea",
   "metadata": {},
   "source": [
    "The adjacency matrix is precisely reconstructed with the embedding. Now, let's drop bottom 5 eigenvectors with the smallest eigenvalues in magnitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7944b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the adjacency matrix\n",
    "Ahat = U[:, :-5] @ V[:, :-5].T\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"ticks\")\n",
    "fig, axes = plt.subplots(figsize=(10, 4), ncols=2)\n",
    "\n",
    "sns.heatmap(A.toarray(), ax=axes[0], cmap=\"cividis\")\n",
    "sns.heatmap(Ahat, ax=axes[1], cmap=\"cividis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2990c3dd",
   "metadata": {},
   "source": [
    "Can you find any difference? How about dropping the top five eigenvectors associated with the largest eigenvalues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec40193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the adjacency matrix\n",
    "Ahat = ...  # construct the adjacency matrix by yourself\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"ticks\")\n",
    "fig, axes = plt.subplots(figsize=(10, 4), ncols=2)\n",
    "\n",
    "sns.heatmap(A.toarray(), ax=axes[0], cmap=\"cividis\")\n",
    "sns.heatmap(Ahat, ax=axes[1], cmap=\"cividis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e40de0",
   "metadata": {},
   "source": [
    "You should be able to spot the differences between the original and reconstructed adjacency matrix more easily this time. In fact, the matrix can be reconstructed well without the bottom eigenvectors with small eigenvalues in magnitude. However, missing few top eigenvectors can lead to a substantial reconstruction error. \n",
    "\n",
    "Let's confirm it quantitatively by computing the reconstruction error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e92898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = A @ U\n",
    "\n",
    "Ahat = U[:, :-5] @ V[:, :-5].T\n",
    "error = np.sum((A.toarray() - Ahat) ** 2) / 2\n",
    "print(f\"Reconstruction error without the bottom 5 eigenvectors: {error:e}\")\n",
    "\n",
    "Ahat = U[:, 5:] @ V[:, 5:].T\n",
    "error = np.sum((A.toarray() - Ahat) ** 2) / 2\n",
    "print(f\"Reconstruction error without the bottom 5 eigenvectors: {error:e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bfd092",
   "metadata": {},
   "source": [
    "ðŸš¨ðŸš¨Math AlertðŸš¨ðŸš¨\n",
    "\n",
    "For those with math phovia, just take a breath. Just take the first two lines text which are sufficient to complete the assignment.  \n",
    "\n",
    "The reconstruction error and eigenvalues are tightly related as follows:\n",
    "$$\n",
    "\\text{Error} = \\frac{1}{2}\\left(\\sum_{i,j}A_{ij}^2 - \\sum_{k=1}^K \\lambda^2\\right)\n",
    "$$ \n",
    "\n",
    "(For those without math phovia, the following is optional, though, I believe that it is a good exercise to be a friend with spectral decomposition. It appears virtually every corner of science and engineering, and highly useful for so many things. It is definitely a good time investment, I believe.)\n",
    "\n",
    "This is because of the following reasons. Denoted by $U$ and $V$ the eigenvectors of the matrix $X$, and $\\Lambda$ be the diagonal matrix with diagonal entries $D_{kk}$ being the eigenvalue $\\lambda_k$.\n",
    "Geometrically speaking $U$ is a rotation matrix for $N$ dimensional space ($N$ is the number of nodes). Since it just rotates the space, it does not shrink or expand the space, i.e., the length is preserved. Therefore, \n",
    "$$\n",
    "J = \\frac{1}{2} || A - \\hat A||_2^2 = \\frac{1}{2} || U^\\top (A - \\hat A) U||_2^2.\n",
    "$$ \n",
    "In fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c66685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = np.sum((A - Ahat) ** 2) / 2\n",
    "error_with_rotation = np.sum((U.T @ (A - Ahat) @ U) ** 2) / 2\n",
    "print(f\"Reconstruction error: {error:e} (with rotation: {error_with_rotation:e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028c226d",
   "metadata": {},
   "source": [
    "Now, let's consider using the top $K$ eigenvectors associated with the largest eigenvalues in magnitude. Denoted by $\\hat U$ and $\\hat V$ the left and right eigenvectors, which are $K$ dimensional. And denoted by $\\hat \\Lambda$ the corresponding eigenvalue matrix. The reconstructed matrix $\\hat A$ is given by \n",
    "$$\n",
    "\\hat A = \\hat U  \\hat \\Lambda  \\hat V\n",
    "$$\n",
    "And the eigenvectors are orthogonal to each other ($\\hat U^\\top \\hat U = I$ and $\\hat U^\\top \\hat V = I$). Thus, we have\n",
    "$$\n",
    "\\hat U^\\top \\hat A \\hat U = \\hat \\Lambda\n",
    "$$\n",
    "In fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73732d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "lam, V = np.linalg.eig(A.toarray().T)\n",
    "V = np.real(V)\n",
    "lam = np.real(lam)\n",
    "\n",
    "Uhat = U[:, :K]\n",
    "Vhat = V[:, :K]\n",
    "lamhat = lam[:K]\n",
    "Ahat = Uhat @ np.diag(lamhat) @ Vhat.T\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"ticks\")\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "sns.heatmap(Uhat.T @ A @ Uhat, cmap=\"RdBu\", center=0, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80134b28",
   "metadata": {},
   "source": [
    "The bottom eigenvectors not included in $\\hat U$ are orthogonal to $\\hat U$. Thus, we have  \n",
    "$$\n",
    "U^\\top \\hat A U = \\text{diag}(\\lambda_1,\\ldots, \\lambda_K, 0,\\ldots, 0) \n",
    "$$\n",
    "(Note that it's $U$ not $\\hat U$). In fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0764505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"ticks\")\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "sns.heatmap(U.T @ Ahat @ U, cmap=\"RdBu\", center=0, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b9283",
   "metadata": {},
   "source": [
    "Altogether, \n",
    "$$\n",
    "\\begin{align}\n",
    "J &= \\frac{1}{2} || A - \\hat A||_2^2 \\\\\n",
    "& = \\frac{1}{2} || U^\\top (A - \\hat A) U||_2^2 \\\\\n",
    "&= \\frac{1}{2} || U^\\top A U - U^\\top \\hat A U||_2^2 \\\\\n",
    "&= \\frac{1}{2} || \\text{diag}(\\lambda_1, \\lambda_2,\\ldots, \\lambda_N) - \\text{diag}(\\lambda_1,\\ldots, \\lambda_K, 0, \\ldots, 0)||_2^2 \\\\\n",
    "&= \\frac{1}{2}\\sum_{k=k+1}^n \\lambda_k ^2 \\\\\n",
    "&= \\frac{1}{2} \\left( ||A||_2^2 - \\sum_{k=1}^K \\lambda_k ^2 \\right)\n",
    "\\end{align}\n",
    "$$ \n",
    "The last equation is derived from the fact $||A||_2^2 = \\sum_{k=1}^N \\lambda_k^2$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advnetsci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
